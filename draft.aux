\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Bouwmans201431}
\citation{Barnich2011_2011_TIP}
\citation{Stauffer1999}
\citation{lee2005}
\citation{Elgammal2000Non}
\citation{Mittal2004KDE}
\citation{Shelhamer2017fcn}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Demonstration of deep variation transformation. Due to the complexity of natural scenes, the original pixels' variation is hard to classify correctly. After transforming by a deep learning network, the pixels in variation become easy to be correctly classified as foreground and background correctly.}}{1}{figure.1}}
\newlabel{idea}{{1}{1}{Demonstration of deep variation transformation. Due to the complexity of natural scenes, the original pixels' variation is hard to classify correctly. After transforming by a deep learning network, the pixels in variation become easy to be correctly classified as foreground and background correctly}{figure.1}{}}
\citation{Stauffer1999}
\citation{Goyal2018}
\citation{Zivkovic2004}
\citation{Kim2005}
\citation{Elgammal2000Non}
\citation{Barnich2011_2011_TIP}
\citation{Hofmann2012Background}
\citation{2015_PR_Varadarajan20153488}
\citation{2017_TPAMI_GANGWANG}
\citation{Sheikh2005Bayesian}
\citation{Reddy2010Robust}
\citation{Izadi2008Robust}
\citation{Chen2011}
\citation{Han2012}
\citation{Sheikh2005Bayesian}
\citation{2012_TIP_OP_6099616}
\citation{Zhang2014Statis}
\citation{Han2012}
\citation{Mobahi2009}
\citation{LANGKVIST201411}
\citation{wang2016PRL}
\citation{Braham2016deep}
\citation{Babaee2017deep}
\citation{St-Charles2015SuBSENSE}
\citation{Wang2014FTSG}
\citation{Yang2018DBMF}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}}
\newlabel{sec2}{{II}{2}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}pixel-based methods}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Region-based approaches}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Machine Learning based Methods}{2}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {III}VARIATION TRANSFORMATION}{3}{section.3}}
\newlabel{sec3}{{III}{3}{VARIATION TRANSFORMATION}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of the original pixel variation, groundtruth and transformed variation. They are represented by the blue line, red line and green line respectively.}}{3}{figure.2}}
\newlabel{variation_chart}{{2}{3}{Comparison of the original pixel variation, groundtruth and transformed variation. They are represented by the blue line, red line and green line respectively}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Foreground Detection via Deep Variation Transformation}{3}{section.4}}
\newlabel{sec4}{{IV}{3}{Foreground Detection via Deep Variation Transformation}{section.4}{}}
\newlabel{eq_reshape}{{2}{4}{Training Data Generation}{equation.4.2}{}}
\citation{CDN2014}
\citation{CAMO}
\citation{Bianco2017TEC}
\citation{St-Charles2015SuBSENSE}
\citation{Jiang2017WeSamBE}
\citation{Chen2015SharedModel}
\citation{Charles2015PAWCS}
\citation{Braham2017Semantic}
\citation{RAMIREZALONSO2016990}
\citation{Babaee2017deep}
\citation{Yang2018DBMF}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Original videos broken down into pixel patches for training of the FCN, which produces an efficient machine for end-to-end transformation learning.}}{5}{figure.3}}
\newlabel{flow_chart}{{3}{5}{Original videos broken down into pixel patches for training of the FCN, which produces an efficient machine for end-to-end transformation learning}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Foreground Detection}{5}{subsection.4.1}}
\newlabel{piecewise_fg}{{8}{5}{Foreground Detection}{equation.4.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experiments}{5}{section.5}}
\newlabel{sec5}{{V}{5}{Experiments}{section.5}{}}
\citation{Babaee2017deep}
\citation{Bianco2017TEC}
\citation{Wang2014FTSG}
\citation{RAMIREZALONSO2016990}
\citation{Gregorio2014CwisarDH}
\citation{Charles2015PAWCS}
\citation{St-Charles2015SuBSENSE}
\citation{Braham2017Semantic}
\citation{Multimode_Background_Subtraction}
\citation{2017_TCSVT_BG_7938679}
\citation{2015_ICME_ShareModel}
\citation{Zivkovic2004}
\citation{Varadarajan2013}
\citation{Stauffer1999}
\citation{WU2010739}
\citation{Barnich2011_2011_TIP}
\citation{Hofmann2012Background}
\citation{Yang2016P2M}
\citation{Yang2018DBMF}
\citation{ZIVKOVIC2006773}
\citation{Baf2008FCI}
\citation{LBA-SOM2008}
\citation{ML-BGS2007}
\citation{DECOLOR2013}
\citation{SHAKERI201628}
\citation{Li2018CAMO}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The qualitative evaluation of the proposed method. All the results is followed in the CDnet 2014.}}{7}{figure.4}}
\newlabel{results_chart}{{4}{7}{The qualitative evaluation of the proposed method. All the results is followed in the CDnet 2014}{figure.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CDnet 2014. For each video, 100 frames were taken for the training of our FCN.}}{7}{table.1}}
\newlabel{tab1}{{I}{7}{The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CDnet 2014. For each video, 100 frames were taken for the training of our FCN}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{7}{section.6}}
\newlabel{sec6}{{VI}{7}{Conclusion}{section.6}{}}
\bibstyle{IEEEtran}
\bibdata{ref}
\bibcite{Bouwmans201431}{1}
\bibcite{Barnich2011_2011_TIP}{2}
\bibcite{Stauffer1999}{3}
\bibcite{lee2005}{4}
\bibcite{Elgammal2000Non}{5}
\bibcite{Mittal2004KDE}{6}
\bibcite{Shelhamer2017fcn}{7}
\bibcite{Goyal2018}{8}
\bibcite{Zivkovic2004}{9}
\bibcite{Kim2005}{10}
\bibcite{Hofmann2012Background}{11}
\bibcite{2015_PR_Varadarajan20153488}{12}
\bibcite{2017_TPAMI_GANGWANG}{13}
\bibcite{Sheikh2005Bayesian}{14}
\bibcite{Reddy2010Robust}{15}
\bibcite{Izadi2008Robust}{16}
\bibcite{Chen2011}{17}
\bibcite{Han2012}{18}
\bibcite{2012_TIP_OP_6099616}{19}
\bibcite{Zhang2014Statis}{20}
\bibcite{Mobahi2009}{21}
\bibcite{LANGKVIST201411}{22}
\bibcite{wang2016PRL}{23}
\bibcite{Braham2016deep}{24}
\bibcite{Babaee2017deep}{25}
\bibcite{St-Charles2015SuBSENSE}{26}
\bibcite{Wang2014FTSG}{27}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces The performance comparison of the proposed approach and some classical methods and deep-based method DBMF. For each video, 64 frames were taken for the training of our FCN.}}{8}{table.2}}
\newlabel{tab2}{{II}{8}{The performance comparison of the proposed approach and some classical methods and deep-based method DBMF. For each video, 64 frames were taken for the training of our FCN}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CAMO-UOW.}}{8}{table.3}}
\newlabel{tab3}{{III}{8}{The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CAMO-UOW}{table.3}{}}
\@writefile{toc}{\contentsline {section}{References}{8}{section*.3}}
\bibcite{Yang2018DBMF}{28}
\bibcite{CDN2014}{29}
\bibcite{CAMO}{30}
\bibcite{Bianco2017TEC}{31}
\bibcite{Jiang2017WeSamBE}{32}
\bibcite{Chen2015SharedModel}{33}
\bibcite{Charles2015PAWCS}{34}
\bibcite{Braham2017Semantic}{35}
\bibcite{RAMIREZALONSO2016990}{36}
\bibcite{Gregorio2014CwisarDH}{37}
\bibcite{Multimode_Background_Subtraction}{38}
\bibcite{2017_TCSVT_BG_7938679}{39}
\bibcite{2015_ICME_ShareModel}{40}
\bibcite{Varadarajan2013}{41}
\bibcite{WU2010739}{42}
\bibcite{Yang2016P2M}{43}
\bibcite{ZIVKOVIC2006773}{44}
\bibcite{Baf2008FCI}{45}
\bibcite{LBA-SOM2008}{46}
\bibcite{ML-BGS2007}{47}
\bibcite{DECOLOR2013}{48}
\bibcite{SHAKERI201628}{49}
\bibcite{Li2018CAMO}{50}
