\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Bouwmans201431}
\citation{Barnich2011_2011_TIP}
\citation{Shelhamer2017fcn}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The demonstration of deep variation transformation. Due to the complexity of natural scenes, the original pixels' variation is hard to classify correctly. After transforming by deep learning network, the pixels in variation become easy to be classified as foreground and background correctly.}}{1}{figure.1}}
\newlabel{idea}{{1}{1}{The demonstration of deep variation transformation. Due to the complexity of natural scenes, the original pixels' variation is hard to classify correctly. After transforming by deep learning network, the pixels in variation become easy to be classified as foreground and background correctly}{figure.1}{}}
\citation{Stauffer1999}
\citation{Zivkovic2004}
\citation{Kim2005}
\citation{Elgammal2000Non}
\citation{Barnich2011_2011_TIP}
\citation{Hofmann2012Background}
\citation{2015_PR_Varadarajan20153488}
\citation{2017_TPAMI_GANGWANG}
\citation{Sheikh2005Bayesian}
\citation{Reddy2010Robust}
\citation{Izadi2008Robust}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}}
\newlabel{sec2}{{II}{2}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}pixels-based methods}{2}{subsection.2.1}}
\@writefile{tdo}{\contentsline {todo}{you should discuss the difference between the proposed approach and pixel-based method. What is your advantage compared with these previous work? Why they failed and why our method work?}{2}{section*.1}}
\pgfsyspdfmark {pgfid1}{35133534}{38644284}
\pgfsyspdfmark {pgfid4}{38331841}{38641991}
\pgfsyspdfmark {pgfid5}{38970816}{38423101}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}region-based approach}{2}{subsection.2.2}}
\@writefile{tdo}{\contentsline {todo}{disccuss the adavantage of proposed work.}{2}{section*.2}}
\pgfsyspdfmark {pgfid6}{21177794}{3254844}
\pgfsyspdfmark {pgfid9}{38331841}{3252551}
\pgfsyspdfmark {pgfid10}{38970816}{3033661}
\citation{Zhang2014Statis}
\citation{Han2012}
\citation{Braham2016deep}
\citation{wang2016PRL}
\citation{Babaee2017deep}
\citation{St-Charles2015SuBSENSE}
\citation{Yang2018DBMF}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Machine Learning based Methods}{3}{subsection.2.3}}
\@writefile{tdo}{\contentsline {todo}{Please cite more papers on this paragraph since our work belongs to this category. This paragraph should be the main part of this section.}{3}{section*.3}}
\pgfsyspdfmark {pgfid11}{3876290}{47112942}
\pgfsyspdfmark {pgfid12}{1893825}{47110649}
\pgfsyspdfmark {pgfid13}{2532800}{46891759}
\@writefile{toc}{\contentsline {section}{\numberline {III}VARIATION TRANSFORMATION}{3}{section.3}}
\newlabel{sec3}{{III}{3}{VARIATION TRANSFORMATION}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces It is hard to separate the foreground pixels from the background precisely in a time sequence.}}{3}{figure.2}}
\newlabel{variation_chart}{{2}{3}{It is hard to separate the foreground pixels from the background precisely in a time sequence}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}BACKGROUND SUBTRACTION VIA DEEP VARIATION TRANSFORMATION}{4}{section.4}}
\newlabel{sec4}{{IV}{4}{BACKGROUND SUBTRACTION VIA DEEP VARIATION TRANSFORMATION}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Fully convolutional network can efficiently learn to make dense predictions for per-pixel tasks like semantic segmentation.}}{5}{figure.3}}
\newlabel{flow_chart}{{3}{5}{Fully convolutional network can efficiently learn to make dense predictions for per-pixel tasks like semantic segmentation}{figure.3}{}}
\newlabel{piecewise_fg}{{8}{5}{BACKGROUND SUBTRACTION VIA DEEP VARIATION TRANSFORMATION}{equation.4.8}{}}
\citation{CDN2014}
\citation{Bianco2017TEC}
\citation{St-Charles2015SuBSENSE}
\citation{Jiang2017WeSamBE}
\citation{Chen2015SharedModel}
\citation{Charles2015PAWCS}
\citation{Braham2017Semantic}
\citation{RAMIREZALONSO2016990}
\citation{Babaee2017deep}
\citation{Yang2018DBMF}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experiments}{6}{section.5}}
\newlabel{sec5}{{V}{6}{Experiments}{section.5}{}}
\citation{Babaee2017deep}
\citation{Bianco2017TEC}
\citation{Wang2014FTSG}
\citation{RAMIREZALONSO2016990}
\citation{Gregorio2014CwisarDH}
\citation{Charles2015PAWCS}
\citation{St-Charles2015SuBSENSE}
\citation{Braham2017Semantic}
\citation{Multimode_Background_Subtraction}
\citation{2017_TCSVT_BG_7938679}
\citation{2015_ICME_ShareModel}
\citation{Zivkovic2004}
\citation{Varadarajan2013}
\citation{Stauffer1999}
\citation{WU2010739}
\citation{Barnich2011_2011_TIP}
\citation{Hofmann2012Background}
\citation{Yang2016P2M}
\citation{Yang2018DBMF}
\citation{ZIVKOVIC2006773}
\citation{Baf2008FCI}
\citation{LBA-SOM2008}
\citation{ML-BGS2007}
\citation{DECOLOR2013}
\citation{SHAKERI201628}
\citation{Li2018CAMO}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The qualitative evaluation of the proposed method. All the results is followed in the CDnet 2014.}}{7}{figure.4}}
\newlabel{results_chart}{{4}{7}{The qualitative evaluation of the proposed method. All the results is followed in the CDnet 2014}{figure.4}{}}
\bibstyle{IEEEtran}
\bibdata{ref}
\bibcite{Bouwmans201431}{1}
\bibcite{Barnich2011_2011_TIP}{2}
\bibcite{Shelhamer2017fcn}{3}
\bibcite{Stauffer1999}{4}
\bibcite{Zivkovic2004}{5}
\bibcite{Kim2005}{6}
\bibcite{Elgammal2000Non}{7}
\bibcite{Hofmann2012Background}{8}
\bibcite{2015_PR_Varadarajan20153488}{9}
\bibcite{2017_TPAMI_GANGWANG}{10}
\bibcite{Sheikh2005Bayesian}{11}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CDnet 2014.}}{8}{table.1}}
\newlabel{tab1}{{I}{8}{The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CDnet 2014}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces The performance comparison of the proposed approach and some classical methods and deep-based method DBMF .}}{8}{table.2}}
\newlabel{tab2}{{II}{8}{The performance comparison of the proposed approach and some classical methods and deep-based method DBMF }{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CAMO-UOW.}}{8}{table.3}}
\newlabel{tab3}{{III}{8}{The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CAMO-UOW}{table.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{8}{section.6}}
\newlabel{sec6}{{VI}{8}{Conclusion}{section.6}{}}
\@writefile{toc}{\contentsline {section}{References}{8}{section*.4}}
\bibcite{Reddy2010Robust}{12}
\bibcite{Izadi2008Robust}{13}
\bibcite{Zhang2014Statis}{14}
\bibcite{Han2012}{15}
\bibcite{Braham2016deep}{16}
\bibcite{wang2016PRL}{17}
\bibcite{Babaee2017deep}{18}
\bibcite{St-Charles2015SuBSENSE}{19}
\bibcite{Yang2018DBMF}{20}
\bibcite{CDN2014}{21}
\bibcite{Bianco2017TEC}{22}
\bibcite{Jiang2017WeSamBE}{23}
\bibcite{Chen2015SharedModel}{24}
\bibcite{Charles2015PAWCS}{25}
\bibcite{Braham2017Semantic}{26}
\bibcite{RAMIREZALONSO2016990}{27}
\bibcite{Wang2014FTSG}{28}
\bibcite{Gregorio2014CwisarDH}{29}
\bibcite{Multimode_Background_Subtraction}{30}
\bibcite{2017_TCSVT_BG_7938679}{31}
\bibcite{2015_ICME_ShareModel}{32}
\bibcite{Varadarajan2013}{33}
\bibcite{WU2010739}{34}
\bibcite{Yang2016P2M}{35}
\bibcite{ZIVKOVIC2006773}{36}
\bibcite{Baf2008FCI}{37}
\bibcite{LBA-SOM2008}{38}
\bibcite{ML-BGS2007}{39}
\bibcite{DECOLOR2013}{40}
\bibcite{SHAKERI201628}{41}
\bibcite{Li2018CAMO}{42}
