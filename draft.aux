\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Bouwmans201431}
\citation{Barnich2011_2011_TIP}
\citation{Stauffer1999}
\citation{Elgammal2000Non}
\citation{Shelhamer2017fcn}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The demonstration of deep variation transformation. Due to the complexity of natural scenes, the original pixels' variation is hard to classify correctly. After transforming by deep learning network, the pixels in variation become easy to be classified as foreground and background correctly.}}{1}{figure.1}}
\newlabel{idea}{{1}{1}{The demonstration of deep variation transformation. Due to the complexity of natural scenes, the original pixels' variation is hard to classify correctly. After transforming by deep learning network, the pixels in variation become easy to be classified as foreground and background correctly}{figure.1}{}}
\citation{Stauffer1999}
\citation{Zivkovic2004}
\citation{Kim2005}
\citation{Elgammal2000Non}
\citation{Barnich2011_2011_TIP}
\citation{Hofmann2012Background}
\citation{2015_PR_Varadarajan20153488}
\citation{2017_TPAMI_GANGWANG}
\citation{Sheikh2005Bayesian}
\citation{Reddy2010Robust}
\citation{Izadi2008Robust}
\citation{Han2012}
\citation{Zhang2014Statis}
\citation{Han2012}
\citation{wang2016PRL}
\citation{Braham2016deep}
\citation{Babaee2017deep}
\citation{St-Charles2015SuBSENSE}
\citation{Wang2014FTSG}
\citation{Yang2018DBMF}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}}
\newlabel{sec2}{{II}{2}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}pixel-based methods}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}region-based approach}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Machine Learning based Methods}{2}{subsection.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces It is hard to separate the foreground pixels from the background precisely in a time sequence.}}{3}{figure.2}}
\newlabel{variation_chart}{{2}{3}{It is hard to separate the foreground pixels from the background precisely in a time sequence}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}VARIATION TRANSFORMATION}{3}{section.3}}
\newlabel{sec3}{{III}{3}{VARIATION TRANSFORMATION}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}foreground detection VIA DEEP VARIATION TRANSFORMATION}{3}{section.4}}
\newlabel{sec4}{{IV}{3}{foreground detection VIA DEEP VARIATION TRANSFORMATION}{section.4}{}}
\citation{Shelhamer2017fcn}
\newlabel{piecewise_fg}{{7}{4}{foreground detection VIA DEEP VARIATION TRANSFORMATION}{equation.4.7}{}}
\citation{CDN2014}
\citation{CAMO}
\citation{Bianco2017TEC}
\citation{St-Charles2015SuBSENSE}
\citation{Jiang2017WeSamBE}
\citation{Chen2015SharedModel}
\citation{Charles2015PAWCS}
\citation{Braham2017Semantic}
\citation{RAMIREZALONSO2016990}
\citation{Babaee2017deep}
\citation{Yang2018DBMF}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Original videos break down into pixel patches for the training of the FCN, which Fully convolutional network produces an efficient machine for end-to-end transformation learning.}}{5}{figure.3}}
\newlabel{flow_chart}{{3}{5}{Original videos break down into pixel patches for the training of the FCN, which Fully convolutional network produces an efficient machine for end-to-end transformation learning}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experiments}{5}{section.5}}
\newlabel{sec5}{{V}{5}{Experiments}{section.5}{}}
\citation{Babaee2017deep}
\citation{Bianco2017TEC}
\citation{Wang2014FTSG}
\citation{RAMIREZALONSO2016990}
\citation{Gregorio2014CwisarDH}
\citation{Charles2015PAWCS}
\citation{St-Charles2015SuBSENSE}
\citation{Braham2017Semantic}
\citation{Multimode_Background_Subtraction}
\citation{2017_TCSVT_BG_7938679}
\citation{2015_ICME_ShareModel}
\citation{Zivkovic2004}
\citation{Varadarajan2013}
\citation{Stauffer1999}
\citation{WU2010739}
\citation{Barnich2011_2011_TIP}
\citation{Hofmann2012Background}
\citation{Yang2016P2M}
\citation{Yang2018DBMF}
\citation{ZIVKOVIC2006773}
\citation{Baf2008FCI}
\citation{LBA-SOM2008}
\citation{ML-BGS2007}
\citation{DECOLOR2013}
\citation{SHAKERI201628}
\citation{Li2018CAMO}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The qualitative evaluation of the proposed method. All the results is followed in the CDnet 2014.}}{7}{figure.4}}
\newlabel{results_chart}{{4}{7}{The qualitative evaluation of the proposed method. All the results is followed in the CDnet 2014}{figure.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CDnet 2014. For each video, 100 frames were taken for the training of our FCN.}}{7}{table.1}}
\newlabel{tab1}{{I}{7}{The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CDnet 2014. For each video, 100 frames were taken for the training of our FCN}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{7}{section.6}}
\newlabel{sec6}{{VI}{7}{Conclusion}{section.6}{}}
\bibstyle{IEEEtran}
\bibdata{ref}
\bibcite{Bouwmans201431}{1}
\bibcite{Barnich2011_2011_TIP}{2}
\bibcite{Stauffer1999}{3}
\bibcite{Elgammal2000Non}{4}
\bibcite{Shelhamer2017fcn}{5}
\bibcite{Zivkovic2004}{6}
\bibcite{Kim2005}{7}
\bibcite{Hofmann2012Background}{8}
\bibcite{2015_PR_Varadarajan20153488}{9}
\bibcite{2017_TPAMI_GANGWANG}{10}
\bibcite{Sheikh2005Bayesian}{11}
\bibcite{Reddy2010Robust}{12}
\bibcite{Izadi2008Robust}{13}
\bibcite{Han2012}{14}
\bibcite{Zhang2014Statis}{15}
\bibcite{wang2016PRL}{16}
\bibcite{Braham2016deep}{17}
\bibcite{Babaee2017deep}{18}
\bibcite{St-Charles2015SuBSENSE}{19}
\bibcite{Wang2014FTSG}{20}
\bibcite{Yang2018DBMF}{21}
\bibcite{CDN2014}{22}
\bibcite{CAMO}{23}
\bibcite{Bianco2017TEC}{24}
\bibcite{Jiang2017WeSamBE}{25}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces The performance comparison of the proposed approach and some classical methods and deep-based method DBMF. For each video, 64 frames were taken for the training of our FCN.}}{8}{table.2}}
\newlabel{tab2}{{II}{8}{The performance comparison of the proposed approach and some classical methods and deep-based method DBMF. For each video, 64 frames were taken for the training of our FCN}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CAMO-UOW.}}{8}{table.3}}
\newlabel{tab3}{{III}{8}{The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CAMO-UOW}{table.3}{}}
\@writefile{toc}{\contentsline {section}{References}{8}{section*.1}}
\bibcite{Chen2015SharedModel}{26}
\bibcite{Charles2015PAWCS}{27}
\bibcite{Braham2017Semantic}{28}
\bibcite{RAMIREZALONSO2016990}{29}
\bibcite{Gregorio2014CwisarDH}{30}
\bibcite{Multimode_Background_Subtraction}{31}
\bibcite{2017_TCSVT_BG_7938679}{32}
\bibcite{2015_ICME_ShareModel}{33}
\bibcite{Varadarajan2013}{34}
\bibcite{WU2010739}{35}
\bibcite{Yang2016P2M}{36}
\bibcite{ZIVKOVIC2006773}{37}
\bibcite{Baf2008FCI}{38}
\bibcite{LBA-SOM2008}{39}
\bibcite{ML-BGS2007}{40}
\bibcite{DECOLOR2013}{41}
\bibcite{SHAKERI201628}{42}
\bibcite{Li2018CAMO}{43}
