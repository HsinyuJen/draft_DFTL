\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Shelhamer2017fcn}
\citation{Bouwmans201431}
\citation{CDN2014}
\citation{Stauffer1999}
\citation{Shelhamer2017fcn}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces XXX.}}{1}{figure.1}}
\newlabel{idea}{{1}{1}{XXX}{figure.1}{}}
\citation{Stauffer1999}
\citation{Zivkovic2004}
\citation{Kim2005}
\citation{Elgammal2000Non}
\citation{Barnich2011_2011_TIP}
\citation{Hofmann2012Background}
\citation{2015_PR_Varadarajan20153488}
\citation{2017_TPAMI_GANGWANG}
\citation{Sheikh2005Bayesian}
\citation{Reddy2010Robust}
\citation{Izadi2008Robust}
\citation{Zhang2014Statis}
\citation{Han2012}
\citation{Braham2016deep}
\citation{wang2016PRL}
\citation{Babaee2017deep}
\citation{St-Charles2015SuBSENSE}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}pixels-based methods}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}region-based approach}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}learning-based methods}{2}{subsection.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces It is hard to separate the foreground pixels from the background precisely in a time sequence.}}{3}{figure.2}}
\newlabel{variation_chart}{{2}{3}{It is hard to separate the foreground pixels from the background precisely in a time sequence}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}VARIATION TRANSFORMATION}{3}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}BACKGROUND SUBTRACTION VIA DEEP VARIATION TRANSFORMATION}{3}{section.4}}
\newlabel{piecewise_fg}{{8}{4}{BACKGROUND SUBTRACTION VIA DEEP VARIATION TRANSFORMATION}{equation.4.8}{}}
\citation{CDN2014}
\citation{Bianco2017TEC}
\citation{St-Charles2015SuBSENSE}
\citation{Jiang2017WeSamBE}
\citation{Chen2015SharedModel}
\citation{Charles2015PAWCS}
\citation{Braham2017Semantic}
\citation{RAMIREZALONSO2016990}
\citation{Babaee2017deep}
\citation{Yang2018DBMF}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Fully convolutional networks can efficiently learn to make dense predictions for per-pixel tasks like semantic segmentation.}}{5}{figure.3}}
\newlabel{flow_chart}{{3}{5}{Fully convolutional networks can efficiently learn to make dense predictions for per-pixel tasks like semantic segmentation}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experiments}{5}{section.5}}
\citation{Babaee2017deep}
\citation{Bianco2017TEC}
\citation{Wang2014FTSG}
\citation{RAMIREZALONSO2016990}
\citation{Gregorio2014CwisarDH}
\citation{Charles2015PAWCS}
\citation{St-Charles2015SuBSENSE}
\citation{Braham2017Semantic}
\citation{Multimode_Background_Subtraction}
\citation{2017_TCSVT_BG_7938679}
\citation{2015_ICME_ShareModel}
\citation{Zivkovic2004}
\citation{Varadarajan2013}
\citation{Stauffer1999}
\citation{WU2010739}
\citation{Barnich2011_2011_TIP}
\citation{Hofmann2012Background}
\citation{Yang2016P2M}
\citation{Yang2018DBMF}
\citation{ZIVKOVIC2006773}
\citation{Baf2008FCI}
\citation{LBA-SOM2008}
\citation{ML-BGS2007}
\citation{DECOLOR2013}
\citation{SHAKERI201628}
\citation{Li2018CAMO}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The qualitative evaluation of the proposed method. All the results is followed in the CDnet 2014.}}{7}{figure.4}}
\newlabel{results_chart}{{4}{7}{The qualitative evaluation of the proposed method. All the results is followed in the CDnet 2014}{figure.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CDnet 2014.}}{7}{table.1}}
\newlabel{tab1}{{I}{7}{The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CDnet 2014}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces The performance comparison of the proposed approach and some classical methods and deep-based method DBMF .}}{7}{table.2}}
\newlabel{tab2}{{II}{7}{The performance comparison of the proposed approach and some classical methods and deep-based method DBMF }{table.2}{}}
\bibstyle{IEEEtran}
\bibdata{ref}
\bibcite{Shelhamer2017fcn}{1}
\bibcite{Bouwmans201431}{2}
\bibcite{CDN2014}{3}
\bibcite{Stauffer1999}{4}
\bibcite{Zivkovic2004}{5}
\bibcite{Kim2005}{6}
\bibcite{Elgammal2000Non}{7}
\bibcite{Barnich2011_2011_TIP}{8}
\bibcite{Hofmann2012Background}{9}
\bibcite{2015_PR_Varadarajan20153488}{10}
\bibcite{2017_TPAMI_GANGWANG}{11}
\bibcite{Sheikh2005Bayesian}{12}
\bibcite{Reddy2010Robust}{13}
\bibcite{Izadi2008Robust}{14}
\bibcite{Zhang2014Statis}{15}
\bibcite{Han2012}{16}
\bibcite{Braham2016deep}{17}
\bibcite{wang2016PRL}{18}
\bibcite{Babaee2017deep}{19}
\bibcite{St-Charles2015SuBSENSE}{20}
\bibcite{Bianco2017TEC}{21}
\bibcite{Jiang2017WeSamBE}{22}
\bibcite{Chen2015SharedModel}{23}
\bibcite{Charles2015PAWCS}{24}
\bibcite{Braham2017Semantic}{25}
\bibcite{RAMIREZALONSO2016990}{26}
\bibcite{Yang2018DBMF}{27}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CAMO-UOW.}}{8}{table.3}}
\newlabel{tab3}{{III}{8}{The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CAMO-UOW}{table.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{8}{section.6}}
\@writefile{toc}{\contentsline {section}{References}{8}{section*.1}}
\bibcite{Wang2014FTSG}{28}
\bibcite{Gregorio2014CwisarDH}{29}
\bibcite{Multimode_Background_Subtraction}{30}
\bibcite{2017_TCSVT_BG_7938679}{31}
\bibcite{2015_ICME_ShareModel}{32}
\bibcite{Varadarajan2013}{33}
\bibcite{WU2010739}{34}
\bibcite{Yang2016P2M}{35}
\bibcite{ZIVKOVIC2006773}{36}
\bibcite{Baf2008FCI}{37}
\bibcite{LBA-SOM2008}{38}
\bibcite{ML-BGS2007}{39}
\bibcite{DECOLOR2013}{40}
\bibcite{SHAKERI201628}{41}
\bibcite{Li2018CAMO}{42}
