\relax 
\@writefile{toc}{\contentsline {title}{Lecture Notes in Computer Science}{1}}
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{Chongqing University}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}pixels-based methods}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}region-based approach}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Deep learning methods}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}VARIATION TRANSFORMATION}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces It is hard to separate the foreground pixels from the background precisely in a time sequence.}}{5}}
\newlabel{variation_chart}{{1}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}BACKGROUND SUBTRACTION VIA DEEP VARIATION TRANSFORMATION}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Fully convolutional networks can efficiently learn to make dense predictions for per-pixel tasks like semantic segmentation.}}{7}}
\newlabel{flow_chart}{{2}{7}}
\newlabel{piecewise_fg}{{9}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {5}EXPERIMENTAL RESULTS}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The qualitative evaluation of the proposed method. All the results is followed in the CDnet 2014.}}{11}}
\newlabel{results_chart}{{3}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CDnet 2014.}}{12}}
\newlabel{tab1_res}{{1}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The performance comparison of the proposed approach and some classical methods and deep-based method DBMF .}}{12}}
\newlabel{tab2_res}{{2}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CAMO-UOW.}}{13}}
\newlabel{tab3_res}{{3}{13}}
\bibcite{2014_CSR_reviews}{1}
\bibcite{1997_TPAMI_GAUSS}{2}
\bibcite{1999_CVPR_MoG}{3}
\bibcite{2004_ICIP_CodeBook}{4}
\bibcite{2006_RTI_CodeBook}{5}
\bibcite{2009_ICASSP_ViBe}{6}
\bibcite{2011_TIP_ViBep}{7}
\bibcite{2006_TPAMI_TexBased}{8}
\bibcite{2012_PD_RECTGAUSS}{9}
\bibcite{2011_ICCV_MultiScale_Zaharescu}{10}
\bibcite{2014_CVPR_CDnet_Wang}{11}
\bibcite{2014_ICIP_MultiScaleSpatio_Lu}{12}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{14}}
\bibcite{2013_ICAVSBS_SpatialMoG_Varadarajan}{13}
\bibcite{2012_CVPRW_ViBe_Van}{14}
\bibcite{2013_AVSS_RMoG_Varadarajan}{15}
\bibcite{2013_ICAVSBS_RMoG}{16}
\bibcite{2014_ICIP_MST}{17}
\bibcite{2014_CCPR_RefPaper}{18}
