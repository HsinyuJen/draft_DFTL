\documentclass[journal]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{multirow}
\usepackage[unicode,pdftex]{hyperref}
\usepackage{xcolor}

\ifCLASSINFOpdf
\usepackage[pdftex]{graphicx}
\else
\fi

\usepackage{todonotes} 
\hyphenation{op-tical net-works semi-conduc-tor}

\newcommand{\reffig}[1]{Fig. \ref{#1}}
\newcommand{\refsec}[1]{Section \ref{#1}}
\newcommand{\refeq}[1]{Eq. \ref{#1}}
\newcommand{\reftab}[1]{Table \ref{#1}}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\stodo}[1]{\todo[size=\tiny]{#1}}


\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}


\definecolor{level2}{RGB}{255,255,255}
\definecolor{level3}{RGB}{10,10,10}
\definecolor{revised}{RGB}{100,101,140}
\definecolor{continue}{RGB}{255,0,0}
\definecolor{filltext}{RGB}{0,255,255}

\begin{document}
\title{Foreground Detection via Deep Variation Transformation}

\author{Yongxin Ge, 
        Xinyu Ren, 
        Chenqiu Zhao}


% \authorrunning{Lecture Notes in Computer Science: Chongqing University}


\maketitle



\begin{abstract}
%
% 运动检测的已有方法主要关注于像素值的变化。
Previous approaches to foreground detection generally analyze the variation of pixel observations.
% 本文中，我们分析了像素的变动，并提出了DPVTL方法。
% In this paper, we focus on transforming the variation into a new space where that there is an obvious difference between the patterns of variation produced by the moving objects and the background,
In this paper, we focus on transforming the variation into another space where the entry of variation is easily classified, and a novel foreground detection method called Deep Pixel Variation Transformation Learning (DPVTL) is proposed.
% 一个FCN被用来寻求一个变换，可以使变换过后的像素值线性可分。
A fully convolutional network(FCN) is applied to find a transformation of pixel variations.
% 具体来说，我们用一段像素观测值表示pixel variation,并用作网络的输入。 
In particular,
the pixel variation is represented by the sequence of pixel observations and used as the input of the FCN.
% 接下来，FCN学习像素变动的模式，并接上一个线性分类器给像素打标签。
Then, the FCN is trained to learn the pattern of pixel variations for the transformation, followed by a liner classifier for labeling the pixels as foreground or background.
%    
% XXX 这句话还是有点问题
% 这句话我也没看懂。。。
Benefied from the ingenious utilization of deep learning network leading by our clear cognition of essence about foreground detection problem,
the proposed approach adaptively generate superior peroformances in diversly complex scenes.
%
Comprehensive experiments in several standard benchmarks demonstrate the superiority of proposed approach compared with state-of-the-art methods including both deep learning and traditional methods.
%
\end{abstract}

\begin{IEEEkeywords} 
    foreground detection, Feature Transformation, Deep Learning,
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
%背景检测的背景介绍
Foreground detection as a fundamental problem in computer vision\ \cite{Bouwmans201431} has been discussed over decades with the increasing number of cameras,
which is widely used as the pre-processing step of video processing \cite{Barnich2011_2011_TIP}.
% 传统的方法是怎么做的，有什么问题
Typically, it is recognized as a binary classification task that assigns each pixel in the video stream with a label, 
for either belonging to the background or foreground scene.
Traditional foreground detection algorithms focus on anaylsing the pixel variation, establishing background models with statistical methods such as GMM\cite{Stauffer1999} and KDE\cite{Elgammal2000Non}.
However, due to the unpredictability and rapidity of the pixels' variation in natural scenes,
the variation becomes so unordered which is hard to be analyzed for foreground detection.
Therefore, the foreground detection is still a challenging problem in complex natural scenes.
% 现有的方法在简单场景中效果很好了
%Existed methods have already achieved well performances in the scenes of low diversity or complexity, such as the indoor scenes.
%
% 8.21 Ren: 你原来讲传统做法的时候是说它们关注与像素的变动，这不对啊，它们明明关注的是像素的分布，我们才更注重于变动。
% 8.21 Ren: 还有我看有人说，论文里面不用加's 可以直接名词拼接，就像这样 pixel variation.  F. 名词修饰：在学术文章中，很多时候会用到直接用名词做修饰，而不用’s 或 ……of …… 的形式。常见的这类词有：reaction rate；reaction rate constant；reaction temperature；reaction condition  molecular weight distribution……
%

% 总而言之，foreground detection依然有很多问题
% 8.21 Ren: 这里有个however,上上句也有个however.......怎么搞啊？
% 8.21 Ren: 改到这里，不知道怎么改了，先更你确定一下吧，variation 跟 distribution 有没有区别？我的理解是variation就是有时间顺序的，之前的方法没有考虑道时间顺序性，所以就不能叫做variation.只能叫distribution.
% 8.21 Ren  我觉得我们应该确定的是，传统统计学的方法只关注数据的分布概率，而我们更多考虑的是数据在时序上的变动。
% 8.21 Ren  因为这一段跟你的理解有冲突，所以先商量后再改吧。


% 第二段重点阐述什么是deep variation learning
% 在多样的自然场景中，前景可能产生和背景相似，甚至相同的像素，而这种像素很难去分析
In the diversely natural scenes,
it is possible that the moving objects produce the similar or even the same observations of pixels to that of background.
% similar to their counterparts of background but atually belong to foreground .
% 如图所示，像素观测值C就和背景的值几乎一模一样，
As shown in the \reffig{idea},
the observation C is closely related to the observations which belong to the background. However, it is actually produced by moving objects and should be classified as foreground.
% 大多数情况下，他都会被划分成背景
Unfortunately, in most cases, it is highly possible that the observation C will be falsely classified due to the similarity with their counterparts of background.
% 我们的就是希望把这个variation 转化到更简单的地方去
In this work, we focus on learning the pattern of pixels' variation and transforming the variations into a new space where the observations are easier to classified, rather than learning a classifier for individual pixels.
As shown in the bottom part of the \reffig{idea}, the pattern of fragment consist of observations A-D can be learned by the network and transformed into another fragment where these observations are easily and correctly classified as foreground.
% transformatin 之后，就很容易分类了，然后这就是我们的方法的核心了
Based on this motivation, the Deep Variation Transformation model for foreground detection is proposed.
% 
% %实际应用中的挑战
% \todo{the main purpose of the second paragraph is showing our idea. But be careful that you should not mention any details of our method which is the content of third paragraph. Pls re-write this part according to figure 1. The main content should be the description of figure 1, as well as our idea, insight}
% Existed methods have already achieved well performances in the scenes of low diversity or complexity, such as the indoor scenes. 
% However, foreground detection is still unsolved because of the complexity and the diversity of natural scenes.
% There are many forms of natrual scenes, for example, camera jitter, dynamic background, bad weather, illumination changing, intermittent object motion\cite{CDN2014}. In these situations, the backgrounds are no longer being static, while in fact, the background can be dynamic and complex, which brings some severe challenges to the traditional methods.
% To solve this problem, we bring up a novel conception of variation transformation, wherein the historical observations of each pixel are conceived as a unity, the pixel variation. Each piece of pixel variation contains the complete temporal information of historical observations, which turns out to be our advantage that not only the distribution of observations, but also the temporal coherence plays an important role in the task.
\begin{figure}[!t]	% FIGURE: figure/fig1 
\centering
    \includegraphics[width=\linewidth]{figure/demo.pdf}
    % 图例里描述一下。
    \caption{The demonstration of deep variation transformation. Due to the complexity of natural scenes, the original pixels' variation is hard to classify correctly. After transforming by deep learning network, the pixels in variation become easy to be classified as foreground and background correctly.}
    \label{idea}
\end{figure}
%The demonstration of deep variation transformation. The pixels in the transformed variation become easier to be classified as foreground and background correctly
%本文的想法和优势
% The previous methods have already made great progress in generating some sophisticated background models with given videos. 
% Unfortunately, some instructive clues have been long put aside and neglected, since there is no efficient way to deal with them. 
% For instance, the statistical methods\cite{Stauffer1999} model the pixel-wise distribution of historical observations over time, while having no concerns over the sequential information in a video stream. 

%具体介绍
% XXX 一句话，换一行！！！
% In this paper, we propose a novel Deep Variation Transformation Learning (DVTL) model for foreground detection in diversely natural scenes. 
In the DVTL model, 
the sequence of pixel observations is used to represent the variation and input into the network for learning,
which encode both the intensity distribution and sequential information.
%
Then, a Fully Convolutional Network (FCN)\ \cite{Shelhamer2017fcn} is applied to learn the patterns of the pixel variation and find a transformation which guarantees the linear separability of pixels in the transformed variation.
%
In particular,
we take advantage of the strong learning ability of the deep network to learn an end-to-end representation of the pixel variation in a new space where they can be easily classified to background and foreground.
%
Benefited from the noval framework of variation transformation,
proposed approach works well in diversely complex scenes adaptively.
%
%Moreover,
%since the pixel sequences are extracted from individual pixels, a large number of pixel patches may be extracted from only a single image stack, which promises that sufficient training data can be obtained with limited groundtruth.
% 
% 
% 
% \todo{This paragraph is too short! provide comprehensive details of our work!}
% % The final prediction can be obtained by thresholding of the transformed variation. 
% %Our contributions can be summarized as follows:
% The contridbutions of proposed approach is shown as follows:
% \begin{itemize}
% \item We proposed the pixel observation patch to describe the pixel variation, which encode both the intensity distribution and sequential information over time.
%     The observation patch is obtained by reshaping the vector of a pixel's historical observations, subtracted from the image stacks. 
% \item We propose a deep neural network for variation transformation. 
%     Our FCN network is trained to learn the pixel variation and generate a new representation in a new feature space. 
% \end{itemize}
% 
%文章结构

%The outline of this paper is as follows: In Section II, we give a brief discussion about the early and recent relevant works. 
% XXX 不会用引用可以问我或者曲师兄。
The rest part of this paper is organized as follows: In \refsec{sec2}, we give a brief discussion about the early and recent relevant works. 
%
The details of the variation transformation are presented in \refsec{sec3}. The architecture of network is introduced in \refsec{sec4}, followed by experiments and result comparison in \refsec{sec5} and conclude the paper in \refsec{sec6}. 
% \todo{please revise this part by using command refsec}

\section{Related Work}
\label{sec2}
Over the last few decades, a huge number of foreground detection methods have been proposed,
which are broadly categorized into pixel-based, region-based and learning-based methods.
\subsection{pixel-based methods}
% Ren:重新解释了pixel based methods
Pixel-based methods usually assume the independence between neighboring pxiels,
and utilize the low-level features, such as color or gradients for background subtraction.

%GMM 
% cqzhao: 我没有看到过over time，如果你在trans或者顶会(ICCV,CVPR,ECCV)里找到了论文原话，那你可以用哦。
In particular, the Gaussian Mixture Model (GMM) proposed by Stauffer et al.\ \cite{Stauffer1999} is the most popular approach among pixels-based methods.
% 注意英语和汉语的表达习惯
It utilizes a mixture of weighted Gaussians to model the probability distribution of each pixel in time sequence.
Pixels are considered as background if there exists a Gaussian function includes their values with sufficient evidence. 
%GMM+i
Moreover,
Zivkovic et al.\ \cite{Zivkovic2004} improve the GMM method with the utilization of recursive equation
to automatically update the parameters and adjust the needed number of components of mixture for each pixel. 
%
%codebook
Hence, Kim et al.\cite{Kim2005} present the codebook method, which records the sampling background values to codewords for each pixel position.
The incoming pixels are compared with these codewords to see if their distances lies within a certain bound. 
%KDE
A non-parametric background model is proposed by Elgammal et al. \cite{Elgammal2000Non}. 
They assume that each background pixel is drawn from a probability distribution function, which is estimated with Kernel Density Estimation(KDE). 
%Vibe
Another non-parametric method is proposed by Barnich et al. \cite{Barnich2011_2011_TIP}, called the Visual Background Extractor (ViBe). The background model of ViBe consists of pixel samples from the video stream. Each pixel in the current frame is compared with sampling pixels from the corresponding background model and labelled as the foreground when there exists sufficient samples with a distance to itself within a certain range. 
%PBAS
To adaptively update the parameters, Hofmann et al.\cite{Hofmann2012Background} improve the ViBe by presenting an adaptive threshold, which depended on the pixel position and a background dynamics metric.

%提出其他方法的问题，指出他们的缺陷
Unfortunately,
the pixel-based methods ignore the spatial-temporal information due to their assumption of independence between pixels.
But there is, in fact, a strong coherence in image sequences that contains abundant hidden clues for the background model. 
%
%提出我们的改进,我们认为像素在时序上是互相依赖的
% tight interdependence 明明在google scholar里查得到很多用的好不好...
% To address this shortcoming, 在一篇2018的CVPR的摘要里有啊
% “High Quality Estimation of Multiple Intermediate Frames for Video Interpolation”
To address this shortcoming, we introduce a framework of variation transformation learing, where pixel's historical observations are embedded in a piece of pixel patch and sorted in chronological order as a whole to enter our DPVTL model.
%这样的优点在于保留了数据的完整性，保留了数据的时序相关性。
Bringing together the historical observations ensures the data integrity and preserves the temporal coherence of our training data. 
Moreover, 
the novel framework is capable of learning the patterns of pixel variation and exploiting the spatial-temporal context, compared to those classifiers based on the assumption of pixel independence. 
%所以我们方法比基于像素的要好。
%Hence we demonstrate a comparatively better performance in modelling some challenging scenes, like illumination changing and intermittent object motion.
%\todo{you should discuss the difference between the proposed approach and pixel-based method. What is your advantage compared with these previous work? Why they failed and why our method work?}
%
%
%
%
%
%
\subsection{region-based approach}
% Ren: 好的好的，不乱吹了，我看过有说have similar values的。。。。
Region-based approaches consider the similiary between neighboring pixels, which is utilized to refine the pixel-level classification.
%

%(Regularized region-based)
Varadarajan et al.\ \cite{2015_PR_Varadarajan20153488} proposed a region-based GMM model,
which is derived from expectation maximization theory with the consideration of neighboring pixels.
% Spatiotemporal GMM
In addition, 
Chen et al. \cite{2017_TPAMI_GANGWANG} combine the GMM with constrains of temporal and spatial information from the optical flow and hierarchical superpixels.
%(Bayesian Modeling)
Moreover, 
Sheikh et al.\ \cite{Sheikh2005Bayesian} introduce a framework based on the Markov Random Field modeling with Maximum A Posteriori 
probability (MAP-MRF) estimation, which incorporate the pixel location into background and foreground KDEs for the detection based on spatial context.
%(Robust Foreground Object ICPR2010 Vikas Reddy )
% 这里原文就是用了三层的分类器，不同的方法，一层又一层。
Similarly, in \cite{Reddy2010Robust}, original images are divided into overlapping blocks. Each block is sequentially processed by an adaptive multi-stage classifier, which consists of a likelihood evaluation, an illumination invariant measure and a temporal correlation check.
%( Robust Region-Based ICPR)
Hence, Izadi et al.\cite{Izadi2008Robust} present a robust region-based approach, which generates a pair of foreground maps based on gradient and color respectively. Any foreground region that does not exist in the first foreground map could be recovered from the other one.

In contrast, we accept the assumption that neighboring blocks of background pixels should follow similar variations over time, and combine the pixel variation with its spatial neighbors to revise our prediction. 
Due to the application of deep learning, the proposed approach is more powerful in capturing the structural background variation and achieves significant improvement compared to its competitors.
%
%
%
% \subsection{learning-based methods}
\subsection{Machine Learning based Methods}
The last category of background subtraction methods apply traditional machine learning and deep learning for the foreground detection.

%
%SVM
Traditional machine learning methods are commonly involved with support vector machines (SVM)\cite{Han2012} and Bayesian methods\cite{Zhang2014Statis}. For example, Han et al. \cite{Han2012} integrate gradient, color, and Haar-like features to address the spatiotemporal variations for each pixel. Their background model is obtained for each feature in a kernel density framework and a SVM is employed for classification.

%
Recent years, deep learning methods start to flourish in several computer vision fields. A novel approach for foreground detection with the use of the Convolutional Neural Network(CNN) is proposed by Wang et al.\cite{wang2016PRL}.
They utilize a CNN with a cascade network architecture for segmentation in foreground detection, which perform excellently with sufficient training data.
%
% Ren: 这里我解释一下，为什么看上去这么冗余。这家伙先自己用中值滤波求了个平均背景，然后把平均背景 patches 跟对应的origin image patches 组合作为网络输入，他训练的groundtruth也可以来自其他比如GMM的分类结果图。
Braham et al. \cite{Braham2016deep} employ a scene-specific CNN, which is trained with corresponding image patches from the background image, video frames and groundtruth. In particular, the background image is obtained by temporal median filtering, and the groundtruth can be replaced with segmentation results from other foreground detection methods.
% Ren: 下面这个家伙做法跟上面太类似了，就是背景图像不用中值滤波，而是用其他牛一点的方法去获取。MD
A similar approach is presented by M. Babaee et al. \cite{Babaee2017deep}. Their background images combine the segmentation mask from SuBSENSE\cite{St-Charles2015SuBSENSE} algorithm and the output of Flux Tensor algorithm \cite{Wang2014FTSG}, which is able to adaptively update the parameters used in the background model. They also utilize spatial-median filtering as the post processing of the network predictions.
%DBMF
In \cite{Yang2018DBMF}, a fully convolutional network with the skip architecture is proposed for foreground detection. 
The authors also proposed a temporal approach to sample training images from the given video, thus providing the background model with limited temporal information. 

% cqzhao: 瞎说啥？？？SVM那个就是连续的几帧啊！他们的问题在于， 自然界太复杂，variation太多元，直接分析variation分析不出来，我们学到了variation的pattern，然后转换！
Although some breakthroughs and progress have been made by applying those methods, especially deep learning based approaches with optimized network architechtures. 
However, due to the diversity and complexity of nature scenes, it is still difficult to analyze the pixel variations for classification. 
%我们的不同在于，我们的网络是训练去学习像素变动，并转换到新的空间去进行分类。
Different with their methods, we focus on creating a transformation, which guarantees the linear sparability of pixel variations in the mapping space, rather than building classifiers for individual observations.
%
Besides, our network is trained to learn the patterns of pixel variations, which promises a better performance in time series modeling on temporal coherence.
%
%Moreover, our DPVTL model is implemented without any assistance from other tradition foreground detection methods, hence the reliable performance owing to the noval framework of variation transformation. 


\section{VARIATION TRANSFORMATION}
\label{sec3}
In this section, we will elaborate on the proposed approach and how does the variation transformation work in foreground detection.

%背景检测的难点
foreground detection is essentially an binary classification in time sequence, where pixels in a video stream are separated into two categories: foreground (cars, people or animals) and background (roads, trees or other backdrops). 
In most cases, if we print out the historical observations of a single pixel, it is not hard to see that they usually keep their stability when belonging to the background. 
However, there are some exceptions: historical observations varies regularly when belonging to some dynamic background scenes(e.g. 
waves, swaying tree leaves). 
Generally, pixels from the background are sharing some common patterns of variation. 
\begin{figure}[!t]	% FIGURE: figure/fig1 
\centering
    \includegraphics[width=\linewidth]{figure/fig1}
    % 图例里描述一下。
    \caption{It is hard to separate the foreground pixels from the background precisely in a time sequence.}
    \label{variation_chart}
\end{figure}


%图例解释困难点
To give an intuitive understanding of the pixel variation, we plot the historical observations and corresponding groundtruth of a single pixel in  \reffig{variation_chart}, with the X axis showing their range of variation and the Y axis representing the time. 
As we can see, there are several lines in three colors: blue, red and green. 
The blue line consist of original observations, representing a piece of pixel variation. 
Accordingly, the red line is made of the groundtruth data, corresponding to the observations from the blue one.  
And the green line consist of the outputs of the proposed approach which represent the transformed variation. 

%In practice, foreground detection is to produce a prediction curve, which is as near as possible to the groundtruth carve, by given the pixel variation carve. 

%传统方法的问题
Previous methods have a good performance when background and foreground observations are remarkably different and the backgrounds are normally keeping static. 
While in fact, backgrounds can be rather turbulent and dynamic due to the complexity and diversity of nature scenes. 
Especially when illumination and camouflages are involved, background and foreground observations are easily confused and mixed up. 

%图例说明可能面临的挑战
For example, in \reffig{variation_chart}, it is noticeable that some foreground observations in blue line share the similarly value with background ones. 
In that case, those popular methods will inevitably yield some sticky moments when separating the pixels in the blue line. 
Statistical methods, for instance, are no longer valid, because they only focus on establishing a statistical model for the background, while having little or no concern for the temporal coherence of these observations. 
In other words, despite knowing that pixels from the background share some common patterns of variation in a temporal sequence, they still let the order information of sequential images all go to waste. 



%variation transformation 介绍。
In this paper, a novel framework of pixel variation transformation are proposed. %用framework代替method?
The pixel variation is now regarded as a whole to safeguard data integrity.
Consequently, the classification of pixels can be viewed as a transformation of pixel variations. 
More specifically, a FCN is trained to learn the patterns of pixel variations and mapping them into a new space where it is close to the groundtruth data.
The transformed variation is presented by the green line in \reffig{variation_chart}. 
%After thresholding, we can easily get the labels of each observation. 
%方法的好处是
The benefits from variation transformation are evident and clearly seen. 
It is hard to distinguish a foreground observation when its value are similar to the background ones. 
However, classification on the observations in a transformed variation is much convenient and intuitive, benefit from the strong learning ability of the FCN model. 
%With the aid of deep learning method, the proposed approach can be effectively implemented.

% 公式表达
%And the definition of region searching method $G(x,y)$ is shown as follows:
%\begin{equation}
%    G(x,y) =  \mathop{\argmin}_{}{ \lVert I_t(m,n) - I_b(x,y) \rVert_{1}  } \quad  m,n \in x,y \pm R,
%\end{equation}

%where $x$ and $y$ is the location of pixels. And $I_t$ is the current frame, where $t$ is the
%time index.

%me
\section{foreground detection VIA DEEP VARIATION TRANSFORMATION}
\label{sec4}
In this section, we will introduce the details of proposed approaches. 
We explain the production of pixel patches, which is a specific form of the pixel variation, and the architecture of our network. 


The the flow chart is illustrated in \reffig{flow_chart}. 
Firstly, we sample the input and ground truth images to generalize the pixel variations, and reshape them into patches and feed them into the network with its spatial neighbors. 
After reassembling the patches into the complete output frame, it is post-processed, yielding the final segmentation of the respective video frame.
\begin{figure*}[!t] % FIGURE: figure/fig1 
\centering
\includegraphics[width=\textwidth]{figure/fig2}
\DeclareGraphicsExtensions.
    \caption{ Original videos break down into pixel patches for the training of the FCN, which 
Fully convolutional network produces an efficient machine for end-to-end transformation learning.}
    \label{flow_chart}
\end{figure*}


% 第一步是对原视频进行采样
Given different videos, their numbers of frames are generally different. 
In order to keep each piece of variation the same length, image stacks are sampled form the given videos before the pixel variations are extracted. 
Let denote the frames sequence as $\{ I_1,I_2,I_3,\dots,I_L\}$.
And image stacks can be defined as follows: 
\begin{equation}
Stack_{(x)}=\{I_x,I_{(x+p)},I_{(x+2p)},\dots,I_L \},\quad p\times l=L,\quad 1\leq   x\leq p , 
\end{equation}
% 解释采样的细节
where $I_t$ is the frame $t$ of the given video. 
And $p$ is an integer number depended on the video frames length. 
For each video, we can produce multiple image stacks and choose one of them for the training. 
In addition, we can get compact pixel variations, which contain more temporal information than the continuous pixel sequences, by using equal interval sampling. 

% 第二步提取Pixel variation
After the sampling process, a large number of observation sequences, or pixel variations, are extracted from the chosen stack at each pixel position, which is shown as follows: 
\begin{equation}
Sq_{(m)}=\{P_1^m,P_2^m,P_3^m,\dots,P_n^m\},
\end{equation}
where $P_t^m$ denotes the numerical value of pixel $m$ at frame $t$ in the chosen stack. 
Each of the pixel variation is a piece of temporal information which containing the changing patterns of background pixels. 
And each of them is of fixed length $n$. 
It is notable that observation sequences are extracted from individual pixel positions, which promises that sufficient training data can be obtained with only one image stack.

% 第三步变成矩阵形式，解释为什么要把variation 变形
Our motivation is to provide an end-to-end transformation of pixel variations, based on the strong learning ability of FCNs. 
However, vectors are not appropriate for the network training and learning. 
Besides, we also hope the pixel observations can be interacted with their further compatriots in temporal sequence. 
Thus we reshape the variations into pixel patches as the input of our network. 
A sample of the pixel patch is like this:
\begin{equation}
H^m=P^m_{i+j\times d}=\begin{bmatrix}
 P^m_1& P^m_2  &\dots  &P^m_d \\ 
\vdots &  &  &\vdots \\ 
 P^m_{1+(d-1)d}& \dots & \dots & P^m_n
\end{bmatrix},\quad d^2=n.
\end{equation}
where the pixel patch is denoted as $H^m$, and the parameter $i$ and $j$ is the column and row of the pixel patch. 
Observations are put into the $d\times d$ patch from top to bottom and left to right.
To put it another way, the pixel patch is just a specific form of the pixel variation. 
%Although the observation patch and the observation sequence belong to different forms of the pixel variation, they consist of the same components, and share the common temporal information from the pixel variation. 
Since the pixel sequences are extracted from each pixel position, a large number of pixel patches can be extracted from only one single image stack, which promises sufficient training data for our network.

%第四步空间近邻随机采样。
It is generally accepted that neighboring background pixels share a similar temporal distribution.
%In other words, there are some clues hidden in pixels' spatial neighbors. 
In order to benefit from the spatial context, we concatenate the pixel patch with 2 neighboring pixel patches, which are randomly selected in the 8-connected neighborhood of each pixel position. 
The input patch $I$ of our network is defined as:
\begin{equation}
I = C(H^m, SM_1, SM_2),
\end{equation}
where the $C$ represents the concatenating process on the third dimension.
And neighboring patches are denoted as $SM_1$ and $SM_2$ respectively.

%In the previous steps, original videos are broken down into pixel patches which contain abundant background information. 
Correspondingly, the groundtruth patches are obtained in a similar way as the input ones, except the concatenating process, as shown in the \reffig{flow_chart}. 
Both of them are fed into the network for the training to find a end-to-end transformation at each pixel position. 
However, the size of feature maps will shrink quickly during forward computation. 
In order to make our prediction the same size as the input patches, we borrow the idea from Image Semantic Segmentation \cite{Shelhamer2017fcn}, padding the input patches before the training. 
After the forward computing, observations in input patches are mapped into a new space where they can be easily classified by thresholding. 
The transformed pixel patch is defined as:
\begin{equation}
H_p= \mathcal L (E_x (H)),
\end{equation}
where, $H_p$ is the prediction of the network, and the forward computation of network is represented by $\mathcal L$. The symmetric padding process is denoted as $E_x$, where we have lost none of the pixel information to make the output $H_p$ the same size as the pixel patch $H$. 
Another advantage of symmetric padding is that the order information still remains.

For the loss function, we choose the Sigmoid Cross Entropy (SCE), which are helpful to address the learning slowdown. 
The formulation is as follows:
\begin{equation}
    \begin{aligned}
        & \ell_{H_p, H_{gt}} =  \sum\limits_{x \in X, y \in Y}^{} H_{gt}(x,y) log(S(H_p(x,y)))  \\
        & + (1 - H_{gt}(x,y)) log(1 - S(H_p(x,y))) \\
        & S(x) =\frac{1}{1+e^{-x}}\  ,
    \end{aligned}
\end{equation}
where $H_{gt}$ denotes the groundtruth patch which is given by the corresponding groundtruth stack, and $S$ represents the sigmoid function. 
The SCE is calculated between the transformed patches and the corresponding groundtruth patches. 
Boundaries of moving objects and pixels that out of the region of interest are ignored in the cost function.
Finally, we get the transformed variation through the FCN, which are easier for classification. 

As shown in the \reffig{flow_chart}, the prediction maps can be reconstructed from the prediction patches.
Then a global threshold $r$ is set for each transformed observation in order to map them to $\{0,1\}$. 
The threshold function is given by
\begin{equation}
    \label{piecewise_fg}
    g(x,y) =
 \begin{cases}
  1,  \quad x < y       \\
  0,  \quad otherwise   \\
\end{cases},
\end{equation}

\begin{equation}
F=g(M,r) ,
\end{equation}
where $M$ is the prediction map, and the $F$ denotes the final foreground map given by our DPVTL model.
After the thresholding calculations, our experiment results show that a random initialized FCNs, trained end-to-end on feature learning can achieve the state-of-the-art without further machinery. 

Here we make a detailed introduction to the deep learning model we used and explain why we choose it.

Different with CNNs, FCNs utilize convolutional layers with $1\times 1$ kernels to take the place of fully connected layers, which provides several benefits. 
First and foremost, the size of outputs are adjustable in FCNs, which allows an end-to-end mapping of input patches and prediction patches at each pixel position. 
We hope to determine the label of an observation through the comparison of its compatriot in time sequence. 
There is one more point I ought to touch on, that since the feature maps are no longer need to be converted into vectors, spatial information can be retained. 
The last but not the least, FCNs have been used in sematic segmentation and researchers found FCNs have a strong learning ability which won't lost to the traditional ones. 
Meanwhile, it's also a high efficient computation model. 


Based on above-mentioned factors, FCN is designed as the alternative network architecture in this paper. 
The structure of our FCN for foreground detection is shown in \reffig{flow_chart}. 
The proposed FCN contains 5 convolutional layers and a convolutional layer which has a filter size of $1\times 1$. 
We use the Rectified Linear Unit (ReLU) as activation function after each convolutional layer and the Sigmoid function after the last fully connected layer. 
We do not use any other tricks in our network training and the experiment results prove that the proposed approach is feasible and very effective.


\section{Experiments}
\label{sec5}
In this section, we ran comprehensive experiments to evaluate the performance of the proposed approach on the CDnet 2014 benchmark\cite{CDN2014} and CAMO-UOW\cite{CAMO}. 
The CDnet is the largest dataset for foreground detection so far as we are aware, containing 11 categories with several complexly challenging scenes, such as Dynamic Background, Camera Jitter, Shadow, Night Videos, PTZ and so on. 
The CAMO-UOW is another challenging benchmark which contains 10 high resolution videos. 
For each video, one or two persons appear in the scene with the clothes in the similar color as the background.

The proposed approach is compared with several existing traditional state-of-the-art foreground detection algorithms, including the IUTIS-5\cite{Bianco2017TEC}, the SuBSENSE\cite{St-Charles2015SuBSENSE}, the WeSamBE\cite{Jiang2017WeSamBE}, sharable GMM the SharedModel\cite{Chen2015SharedModel}, word-dictionaries-based method the PAWCS\cite{Charles2015PAWCS}, the SemanticBGS\cite{Braham2017Semantic}, the AAPSA\cite{RAMIREZALONSO2016990}, etc. 
Moreover, two deep learning based algorithms are also compared with the proposed approach, which include DeepBS\cite{Babaee2017deep}, and DBMF\cite{Yang2018DBMF}. 
All the results of compared algorithms are provided by authors.

During the comparison, the F-measure(Fm) has been used for evaluation. 
The Fm is a general international standard in foreground detection which measures the segmentation accuracy by considering both the recall and the precision. 
The definition of Fm is shown as follows:
\begin{equation}
Fm= \frac{2\times precision \times recall}{precision + recall} = \frac{2TP}{2TP+FN+FP},
\end{equation}
% 
% 
% \begin{equation}
%     F=\frac{precision \dot recall}{precision - recall}  = \frac{2TP}{2TP+FN+FP},
% \end{equation}
%
where TP, FP, and FN are true positives, false positives, and false negatives respectively, computed in pixels of all test frames for each video. 


The quantitative and qualitative comparisons are shown in \reftab{tab1} and  \reffig{results_chart} respectively. 
Due to the paper length, several typical videos are selected for the qualitative comparisons as well as the discussion. 
In the dynamic background scene, the video "canoe" is a typically challenging video which includes a large area of water rippling. 
The main challenge comes from the dynamic background, in which it is so hard to describe the background by a single image. 
In this condition, since the traditional foreground detection method such as the SharedModel and the WeSamBE do not have the enough ability to describe the complex dynamically background, they are fail to detect the people on the boat, as shown in the \reffig{results_chart}. 
Besides, the detected moving objects of the SharedModel are not accurate in boundary due to the utilization of texture features. 
In contrast, benefited from the strong learning ability of Deep Learning network, the DeepBS successfully detected the people. 
Unfortunately, since the DeepBS ignores the fact that a single background is not enough to describe the dynamic background, even the deep learning based algorithm is suffering from the detection of the boat shape. 
In contrast, the proposed approach performed superior than others in this scene, since the essence of foreground detection is considered as a binary classification of pixels' observation in time sequence. 
Based on this insight, the FCN network focuses on learning the patterns of the pixel variation rather than a static background image, and proposed approach achieves promising performance in the canoe video.

\begin{figure*}[!t]	\centering
    \includegraphics[width=\textwidth]{figure/fig3}
\DeclareGraphicsExtensions.
    \caption{The qualitative evaluation of the proposed method. All the results is followed in the CDnet 2014.}
    \label{results_chart}
\end{figure*}

As for the case of shadow scene, "peopleInShade" is a typical example with prevalent hard and soft shadows. 
In the traditional approaches, these shadow regions are usually segmented as foreground since it is also moving with the objects. 
Therefore, traditional methods like the PAWCS, the WeSamBE and the SharedModel falsely segments part of shadows as moving objects. 
In addition, the foreground provided by the IUTIS-5 is incomplete on the part of the pedestrian's body due to the interference of shades, which can be owing to the severe dependency of texture features. 
Whereas, the DeepBS performs well in this video benefited from the utilization of CNN. 
However, the shape of pedestrians are slightly deformed as the result of their patch-wise processing of CNN. 
In contrast, derived from the fact that our DPVL focus on learning the pattern of pixels' variation in the shadow regions, proposed approach successfully segments the shadow part as the background and achieves the highest performance in the category of shadow scene.

In the video "corridor" among the Thermal scene, there is no color information since the videos are obtained through a Thermal camera. 
Moreover, the moving objects in these videos are exceedingly fuzzy and indistinct, which is the main challenge of this category. 
The WeSamBE, the SharedModel and the PAWCS successfully detect the target objects, owing to a stable background in this indoor video. 
However, they fail to remove the reflections since their modeling ability have already reached a limit under the extreme condition of thermal map. 
The DeepBS, by contrast, succeeds in eliminating most of the reflections. 
Meanwhile, the moving objects are also clearly divided from the background thanks to the strong modeling ability of CNN. 
However, due to the dependency of edge feature, a small object were missed in the detection result. 
Fortunately, the proposed approach focus on the pattern of pixels' variation, which should be theoretically effective even in the observation without the color information. 
Consequently, our DPVL performed much better than compared algorithms, with the situation that most parts of shadow are removed and the segmentation results are more accurate.

% \begin{table}[tab1]
\begin{table*}[!t]				% TABLE
\centering
\caption{The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CDnet 2014. For each video, 100 frames were taken for the training of our FCN.}
\label{tab1}
\begin{tabular}{lllllllllllll}
\hline
Videos      & baseline & dyna.bg & cam.jitter & int.obj.m & shadow & thermal & bad.weat & low f.rate & night vid. & PTZ    & turbul. & overall \\ \hline
DeepBS\cite{Babaee2017deep}      & 0.9580   & 0.8761  & 0.8990     & 0.6097    & 0.9304 & 0.7583  & 0.8647   & 0.5900     & 0.6359     & 0.3306 & 0.8993  & 0.7458  \\
IUTIS-5\cite{Bianco2017TEC}     & 0.9567   & 0.8902  & 0.8332     & 0.7296    & 0.9084 & 0.8303  & 0.8289   & \textbf{0.7911}     & 0.5132     & 0.4703 & 0.8507  & 0.7717  \\
FTSG\cite{Wang2014FTSG}        & 0.9330   & 0.8792  & 0.7513     & 0.7891    & 0.8832 & 0.7768  & 0.8228   & 0.6259     & 0.5130     & 0.3241 & 0.7127  & 0.7283  \\
AAPSA\cite{RAMIREZALONSO2016990}       & 0.9183   & 0.6706  & 0.7207     & 0.5098    & 0.7953 & 0.7030  & 0.7742   & 0.4942     & 0.4161     & 0.3302 & 0.4643  & 0.6179  \\
CwisarDH\cite{Gregorio2014CwisarDH}    & 0.9145   & 0.8274  & 0.7886     & 0.5753    & 0.8581 & 0.7866  & 0.6837   & 0.6406     & 0.3735     & 0.3218 & 0.7227  & 0.6812  \\
PAWCS\cite{Charles2015PAWCS}       & 0.9397   & 0.8938  & 0.8137     & 0.7764    & 0.8934 & 0.8324  & 0.8059   & 0.6433     & 0.4171     & 0.4450 & 0.7667  & 0.7403  \\
SuBSENSE\cite{St-Charles2015SuBSENSE}    & 0.9503   & 0.8177  & 0.8152     & 0.6569    & 0.8986 & 0.8171  & 0.8594   & 0.6594     & 0.4918     & 0.3894 & 0.8423  & 0.7408  \\
SemanticBGS\cite{Braham2017Semantic} & 0.9604   & \textbf{0.9489}  & 0.8388     & 0.7878    & 0.9244 & 0.8219  & 0.8260   & 0.7888     & 0.5014     & 0.5673 & 0.6921  & 0.7892  \\
MBS\cite{Multimode_Background_Subtraction}         & 0.9287   & 0.7915  & 0.8367     & 0.7568    & 0.8262 & 0.8194  & 0.7980   & 0.6350     & 0.5158     & 0.5520 & 0.5858  & 0.7288  \\
WeSamBE\cite{2017_TCSVT_BG_7938679}     & 0.9413   & 0.7440  & 0.7976     & 0.7392    & 0.8999 & 0.7962  & 0.8608   & 0.6602     & 0.5929     & 0.3844 & 0.7737  & 0.7446  \\
ShareM\cite{2015_ICME_ShareModel}      & 0.9522   & 0.8222  & 0.8141     & 0.6727    & 0.8898 & 0.8319  & 0.8480   & 0.7286     & 0.5419     & 0.3860 & 0.7339  & 0.7474  \\
GMM\cite{Zivkovic2004}         & 0.8245   & 0.633   & 0.5969     & 0.5207    & 0.7370 & 0.6621  & 0.7380   & 0.5373     & 0.4097     & 0.1522 & 0.4663  & 0.5707  \\
RMoG\cite{Varadarajan2013}        & 0.7848   & 0.7352  & 0.7010     & 0.5431    & 0.7212 & 0.4788  & 0.6826   & 0.5312     & 0.4265     & 0.2470 & 0.4578  & 0.5735  \\ \hline
DPVTL        & \textbf{0.9811}   & 0.9329  & \textbf{0.9014}     & \textbf{0.9595}    & \textbf{0.9467} & \textbf{0.9479}  & \textbf{0.8780}   & 0.7818     & \textbf{0.7737}     & \textbf{0.5957} & \textbf{0.9034}  & \textbf{0.8789} \\ \hline
\end{tabular}
\end{table*}

\begin{table*}[!t]
\centering
\caption{The performance comparison of the proposed approach and some classical methods and deep-based method DBMF. For each video, 64 frames were taken for the training of our FCN.}
\label{tab2}
\begin{tabular}{llllllll}
\hline
Methods  & highway & office & Pedestrians & PETS2006 & Fall   & sofa   & overall \\ \hline
GMM\cite{Stauffer1999}      & 0.5788  & 0.2338 & 0.5202      & 0.6011   & 0.8026 & 0.5225 & 0.5432  \\
CodeBook\cite{WU2010739} & 0.8356  & 0.5939 & 0.7293      & 0.7808   & 0.3921 & 0.8149 & 0.6911  \\
ViBe\cite{Barnich2011_2011_TIP}     & 0.7535  & 0.6676 & 0.8367      & 0.6668   & 0.6829 & 0.4298 & 0.6729  \\
PBAS\cite{Hofmann2012Background}     & 0.8071  & 0.6839 & 0.7902      & 0.7280   & 0.3420 & 0.5768 & 0.6547  \\
P2M\cite{Yang2016P2M}      & 0.9160  & 0.3849 & 0.9121      & 0.7322   & 0.5819 & 0.4352 & 0.6604  \\
DBMF\cite{Yang2018DBMF}     & 0.9412  & 0.9236 & 0.8394      & 0.9059   & 0.8203 & 0.8645 & 0.8824  \\ \hline
DPVTL     & \textbf{0.9888}  & \textbf{0.9819} & \textbf{0.9728}      & \textbf{0.9808}   & \textbf{0.9394} & \textbf{0.9333} & \textbf{0.9662}  \\ \hline
\end{tabular}
\end{table*}

    
    \begin{table*}[!t]
\centering
\caption{The performance comparison of the proposed approach and some state-of-the-art algorithms on the video sequences from different categories in CAMO-UOW.}
\label{tab3}
\begin{tabular}{lllllllllll}
\hline
Methods  & MOG2\cite{ZIVKOVIC2006773} & FCI\cite{Baf2008FCI}  & LBA-SOM\cite{LBA-SOM2008} & PBAS & SuBSENSE & ML-BGS\cite{ML-BGS2007} & DECOLOR\cite{DECOLOR2013}       & COROLA\cite{SHAKERI201628} & FWFC\cite{Li2018CAMO}          & Ours          \\ \hline
Video 1  & 0.79 & 0.88 & 0.8     & 0.9  & 0.89     & 0.89   & 0.92          & 0.8    & 0.94 & \textbf{0.96} \\
Video 2  & 0.82 & 0.79 & 0.8     & 0.82 & 0.88     & 0.8    & 0.83          & 0.58   & 0.96          & \textbf{0.98} \\
Video 3  & 0.88 & 0.86 & 0.85    & 0.91 & 0.9      & 0.8    & 0.9           & 0.82   & 0.94 & \textbf{0.95} \\
Video 4  & 0.89 & 0.9  & 0.76    & 0.93 & 0.78     & 0.88   & 0.95          & 0.87   & 0.94          & \textbf{0.98} \\
Video 5  & 0.84 & 0.86 & 0.82    & 0.83 & 0.82     & 0.8    & 0.82          & 0.75   & 0.91          & \textbf{0.98} \\
Video 6  & 0.93 & 0.87 & 0.77    & 0.95 & 0.92     & 0.95   & 0.97	      & 0.72   & 0.94          & \textbf{0.98}  \\
Video 7  & 0.76 & 0.83 & 0.88    & 0.91 & 0.87     & 0.79   & 0.91          & 0.83   & 0.96          & \textbf{0.99} \\
Video 8  & 0.83 & 0.87 & 0.85    & 0.87 & 0.93     & 0.86   & 0.86          & 0.68   & \textbf{0.96}          & \textbf{0.96} \\
Video 9  & 0.89 & 0.9  & 0.87    & 0.84 & 0.92     & 0.87   & 0.86          & 0.78   & 0.88          & \textbf{0.99} \\
Video 10 & 0.89 & 0.86 & 0.89    & 0.91 & 0.92     & 0.9    & 0.94          & 0.85   & 0.96          & \textbf{0.97} \\ \hline
average  & 0.85 & 0.86 & 0.83    & 0.89 & 0.88     & 0.85   & 0.90          & 0.77   & 0.94          & \textbf{0.97} \\ \hline
\end{tabular}
\end{table*}




The quantitative evaluation of proposed approach on CDnet 2014 is shown in the \reftab{tab1}. 
It can be inferred that the proposed approach significantly outperformed all of the compared state-of-the-art algorithms in most of complex scenes and achieved 11.37\% gain in FM over the second one on the whole dataset. 
Moreover, in order to compare proposed approach with the DBMF, which is also based on deep learning and only publish their results in  specific videos. 
The proposed approach has also ran in these video and the results are shown in Table 2. 
Again, the proposed approach has noticeably better performance than the DBMF and some other classical background approaches.

	As shown in the \reftab{tab1} and \reftab{tab2}, previous deep learning based methods like the DeepBS and the DBMF achieve well performance. 
From our own perspective, that good performance should attribute to the stronger modeling ability and learning adaptation of CNN and FCN. 
However, the proposed approach focused on the pixels variation in temporal sequence rather than low-level static features such as color, edges and textures, which gives us the ability to avoid the shortcomings of the background models. 
Consequently, the proposed approach still get considerably better results, which over 17.85\% in FM metrics compared with the DeepBS and over 9.50\% compared with the DBMF.
The evaluation of proposed approach in CAMO-UOW dataset is shown in the \reftab{tab3}. 
Unlike the CDnet dataset, the videos of CAMO-UOW dataset are specially proposed for the moving objects with camouflage, which is the main challenge of this dataset. 
As shown in the \reftab{tab3} proposed approach achieves better performance compared to its competitions, with an average F-measure of 0.97, compared to values between 0.77 and 0.94 for the other methods. 
Therefore, it is fair to say that proposed approach performs better compared with their peers. 


In this dataset, target objects have the similar color and textures with the background, which brings a lot of difficulties and obstacles to traditional methods. 
However, our FCN is a powerful Neural Network model which is good at capturing the non-linearities of the manifold of pixel variations. 


All these experiments of the proposed method were implemented in matlab and ran on the computer with Nvidia tasela K80 GPU and all images are keep their original resolution. 
For each video in CDnet 2014, 100 training frames are extracted to produce the image patch. 
It should be noted that the 100 frames only accounts for less than 10\% of total Groundtruth in CDnet 2014. 
In contrast, 90\% of data were used as training samples in the DBMF, which suggest that the proposed approach achieves well performance with limited training frames. 
Considering that the videos in CAMO-UOW have fewer frames, we reduce the number of training frames to 64 for each video. 
During the experience, the training set and testing set are completely separated. 
More specifically, our FCN network is random initialized. 
We train the network with mini-batches of size 200, a learning rate $\alpha  = 1 \times 10^{{−3}}$ over 20 epochs. 
The last threshold $R$ is set to 0.6.


% \begin{table}[!t]				% TABLE
%     \caption{The example of latex table}
% \label{tab_FBMS_nobk}
% \centering
%     \begin{tabular}{|l@{  }c@{  }c@{  }cc@{  }c@{  }c@{  }c|}
% \hline
%      \multirow{2}{*}{Videos} & \multicolumn{3}{c}{$\text{IFB}_{nobk}$} &  & \multicolumn{3}{c|}{IFB} \\
%     \cline{2-4} \cline{6-8}
%        & Re &Pr & Fm &  &  Re & Pr & Fm  \\
% \hline
% cats03          &   (\textbf{0.9472} ,\   &  0.5428 ,\  &   0.6901 )   &  &    (0.9063 ,\	 &  \textbf{0.7955} ,\	 &  \textbf{0.8473} )   \\
% bear01          &   (\textbf{0.9854} ,\   &  0.4806 ,\  &   0.6461 )   &  &    (0.8695 ,\	 &  \textbf{0.8673} ,\	 &  \textbf{0.8684} )   \\
% rabbits01       &   (\textbf{0.9530} ,\   &  0.4433 ,\  &   0.6051 )   &  &    (0.9510 ,\	 &  \textbf{0.8730} ,\	 &  \textbf{0.9103} )   \\
% cars5           &   (\textbf{0.9887} ,\   &  0.3693 ,\  &   0.5378 )   &  &    (0.9437 ,\	 &  \textbf{0.7108} ,\	 &  \textbf{0.8109} )   \\
% tennis          &   (\textbf{0.9364} ,\   &  0.4558 ,\  &   0.6132 )   &  &    (0.8808 ,\	 &  \textbf{0.7345} ,\	 &  \textbf{0.8010} )   \\
% farm01          &   (\textbf{0.9887} ,\   &  0.3897 ,\  &   0.5591 )   &  &    (0.8954 ,\	 &  \textbf{0.7462} ,\	 &  \textbf{0.8140} )   \\
% \hline                                                                                                           
% Average         &   (\textbf{0.9666} ,\   &  0.4469 ,\  &   0.6086 )   &  &    (0.9078 ,\    &  \textbf{0.7879} ,\   &  \textbf{0.8420} )   \\
% \hline
% \end{tabular}
% \end{table}

% Videos & GBSSP\cite{Lim2014_2014_ECCV} & calMoSeg\cite{2016_ECCV_Bideau2016}  & MLayer\cite{2017_ICCV_zhu2017multilayer}  & $\text{IFB}_{SU}$ & $\text{IFB}_{KA}$ & $\text{IFB}_{SI}$ & Videos & GBSSP\cite{Lim2014_2014_ECCV} & calMoSeg\cite{2016_ECCV_Bideau2016} & MLayer\cite{2017_ICCV_zhu2017multilayer}  &  $\text{IFB}_{SU}$ &  $\text{IFB}_{KA}$  &  $\text{IFB}_{SI}$  \\




\section{Conclusion}
\label{sec6}
In this paper, we proposed a noval foreground detection approach based on deep learning and the variation transformation learning. The DPVTL model include a FCN network and a noval variation transformation learing framework, which allows us to efficiently combine the temporal coherence and distribution information of pixels over a long period of time. Comparison with other traditional and deep learning methods shows that the DPVTL has good properties on several challenging scenes.
The future work will be focused on improving the DPVTL in terms of network architecture updating and integration of local spatial coherence.

% \section{Conclusions}
% In this paper,
% we proposed the IFB framework for foreground detection for the case of a freely moving camera.
% Unlike previous work in which attempts were made to improve the accuracy of the estimation of motion,
% our IFB focuses on integrating rough foreground and background cues for foreground segmentation.
% In particular, 
% foreground cues are detected by a GMM model with the estimation of background motion,
% while background cues are captured from spatio-temporal features filtered by homography transformation,
% where the SURF\cite{2006_SURF}, KAZE\cite{2012_KAZE}, SIFT\cite{lowe2004distinctive} features are used as examples.
% % the geometric constraints between the SIFT features.
% Then, super-pixels under multiple levels are utilized to integrate these cues.
% The efficiency of IFB results from the complementarity between foreground and background cues.
% The accuracy of the proposed approach is improved though the utilization of super-pixels under multiple levels.
% A comprehensive experiment to compare our results with the state-of-the-art shows the efficiency of our framework and points to its potential for use in practical applications.
% 
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}  
\bibliography{ref}  


\end{document}
